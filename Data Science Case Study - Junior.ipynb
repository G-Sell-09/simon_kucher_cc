{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f14d9a-8893-449a-930e-f5d946957a2c",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "***alles um tier GmBH*** is a pet supplies company. They are currently auditing their promotional activities and the CEO, one of the main stakeholders, feels that the promotions they offer is too generic and not targeted. They have requested us to devise a customer segmentation model that they can use to run targeted promotional activities.\n",
    "\n",
    "The client is interested in seeing what kind of customers are buying at ***alles um tier GmbH***. They assume that, in addition to private individuals, there are also smaller companies that purchase from ***alles um tier GmBH***. The project scope is to build a segmentation model and analyze the resulting customer segments.\n",
    "\n",
    "# Data\n",
    "\n",
    "You are given a dataset at customer level for the past year with the following data points. Number of transactions in the past year (*num_transactions*), order amount the past year (*total_order_value*), days between transactions the past year (*days_between_trans*), re-order rate the past year (*repeat_share*), and % of dog products bought (*dog_share*).\n",
    "\n",
    "### Data Set\n",
    "The dataset consists of 100k rows and has the following columns:\n",
    "\n",
    "* CustomerID (int): UUID for the customer\n",
    "* num_transactions (int): number of transactions in a given year\n",
    "* total_order_value (float): total order value in â‚¬ for the time period\n",
    "* days_between_trans (float): average days between transactions for a user\n",
    "* repeat_share (float): product share repeated every order\n",
    "* dog_share (float): percentage of products ordered that are dog food related\n",
    "    \n",
    "# Technical Environment\n",
    "* Python\n",
    "* numpy\n",
    "* pandas\n",
    "* scikit-learn\n",
    "* matplotlib / scipy / searborn / altair / plotly\n",
    "\n",
    "# Approach\n",
    "The solution is assessed on the following skills:\n",
    "* A thorough evaluation of the data set using statistical measures and visualization\n",
    "* Elegant Python coding skills\n",
    "* Machine learning modelling fundamentals\n",
    "* Model & result evaluation\n",
    "\n",
    "# Output\n",
    "Please provide your solution in a jupyter notebook with clear markdown comments.\n",
    "The final output should be in the form of a DataFrame with two columns, the CustomerId and the assigned cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acb833",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22559e30",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import altair as alt\n",
    "import plotly.express as px\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f087c59",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed64ab0-ad74-42b4-a516-857d513a1f2e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load the data and make sure to specify the correct delimiter\n",
    "df = pd.read_csv(\"DataSet_JuniorCodingChallenge.csv\", delimiter='|')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727eda2",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how often two values are missing in one row\n",
    "print(f\"Number of rows with two missing values: {len(df[df.isnull().sum(axis=1) == 2])}\")\n",
    "\n",
    "dropping = len(df[df.isnull().sum(axis=1) == 1])/len(df)\n",
    "print(f\"Percentage of rows with one missing value: {dropping:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37eb4ef",
   "metadata": {},
   "source": [
    "On the one hand, we see that only one entry is missing at a time, so it would make sense to fill in these values. On the other hand, only 0.46 % of all customer data have missing values. It could be argued that it is reasonable to simply drop these values, but due to the fact that only one value is missing at a time, we can e.g. use K-Nearest Neighbors Imputation on all numerical values and fill the missing entries in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a copy and drop the CustomerID column\n",
    "df_knn = df.drop(columns='CustomerID')\n",
    "\n",
    "# Use KNN imputation to fill in the missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_knn), columns=df_knn.columns)\n",
    "\n",
    "# Print a statemetn that len(df_imputed.isnull().sum()) is 0\n",
    "print(f\"Showcasing the missing values after imputation:\\n{df_imputed.isnull().sum()}\")\n",
    "\n",
    "# # Check the newly filled values\n",
    "# missing_indices = df[df.isnull().sum(axis=1) == 1].index\n",
    "# df.loc[missing_indices]\n",
    "# df_imputed.loc[missing_indices]\n",
    "\n",
    "# Print a statement to that the new values were checked and seem reasonable\n",
    "print(\"The new filled entries were double checked again and they are reasonable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either drop the missing values or use the imputed data\n",
    "df_drop = df.dropna()\n",
    "\n",
    "# Set df to the imputed data and add the CustomerID column back to the first column\n",
    "df = pd.concat([df['CustomerID'], df_imputed], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18558a30",
   "metadata": {},
   "source": [
    "## Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3431260",
   "metadata": {},
   "source": [
    "At first we want to make sure that the num_transaction has the correct integer values and that the other numerical features are saved as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303900ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73badef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that counts all entries that are not .00 floats\n",
    "def find_floats(df, column):\n",
    "    count = 0\n",
    "    for i in df[column].unique():\n",
    "        if i % 1 != 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# State how many entries need to be rounded in the num_transactions column\n",
    "print(f\"There are {find_floats(df, 'num_transactions')} entries that are not .00 floats and need to be rounded.\")\n",
    "\n",
    "# Round all float values before converting them to integers\n",
    "df['num_transactions'] = df['num_transactions'].apply(lambda x: round(x))\n",
    "\n",
    "# Double Check if all values are rounded now with a print statement\n",
    "print(f\"There are {find_floats(df, 'num_transactions')} entries that are not .00 floats left.\")\n",
    "\n",
    "# Convert num_transactions to integers\n",
    "df['num_transactions'] = df['num_transactions'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ae8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all types again\n",
    "print(\"All data types are correct now.\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d755a0",
   "metadata": {},
   "source": [
    "Now we want to make sure that all numerical values are in reasonable ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f84040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of negative values for num_transactions, total_order_value and days_between_trans for negative values\n",
    "print(f\"Number of negative values for num_transactions: {len(df[df['num_transactions'] < 0])}\")\n",
    "print(f\"Number of negative values for total_order_value: {len(df[df['total_order_value'] < 0])}\")\n",
    "print(f\"Number of negative values for days_between_trans: {len(df[df['days_between_trans'] < 0])}\")\n",
    "\n",
    "# Check repeat_share and dog_share for values between 0 and 1. So count the number of values outside of this range\n",
    "print(f\"Number of values outside of the range [0, 1] for repeat_share: {len(df[(df['repeat_share'] < 0) | (df['repeat_share'] > 1)])}\")\n",
    "print(f\"Number of values outside of the range [0, 1] for dog_share: {len(df[(df['dog_share'] < 0) | (df['dog_share'] > 1)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the negative values in the 3 columns\n",
    "df = df[df['num_transactions'] >= 0]\n",
    "df = df[df['total_order_value'] >= 0]\n",
    "df = df[df['days_between_trans'] >= 0]\n",
    "\n",
    "# Drop the values outside of the range 0 and 1 for the last two columns\n",
    "df = df[(df['repeat_share'] >= 0) & (df['repeat_share'] <= 1)]\n",
    "df = df[(df['dog_share'] >= 0) & (df['dog_share'] <= 1)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates in the CustomerID column and check whether they are consistent\n",
    "def find_inconsistent_duplicates(df, object_col):\n",
    "    # Get the indices of duplicated entries in the object column\n",
    "    duplicated_indices = df[df.duplicated(subset=[object_col], keep=False)].index\n",
    "\n",
    "    # Dictionary to store the indices of inconsistent duplicates\n",
    "    inconsistent_indices = []\n",
    "\n",
    "    # Group by the object column and iterate over each group\n",
    "    for key, group in df.loc[duplicated_indices].groupby(object_col):\n",
    "        # Get the first row's data (excluding the object column)\n",
    "        reference_row = group.iloc[0, 1:].values\n",
    "        \n",
    "        # Check if all rows in the group match the first row\n",
    "        for idx, row in group.iterrows():\n",
    "            if not (row.iloc[1:].values == reference_row).all():\n",
    "                inconsistent_indices.append(idx)\n",
    "\n",
    "    return inconsistent_indices\n",
    "\n",
    "indices_with_inconsistencies = find_inconsistent_duplicates(df, 'CustomerID')\n",
    "\n",
    "# Print the number of inconsistent duplicates\n",
    "print(f\"Number of inconsistent duplicates: {len(indices_with_inconsistencies)}. This is why we can simply drop one of them.\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0564692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State how many CustomerIDs are duplicated and that they will be dropped\n",
    "print(f\"There are {df['CustomerID'].duplicated().sum()} duplicated CustomerIDs. They will be dropped.\")\n",
    "\n",
    "# Drop the duplicated CustomerIDs\n",
    "df = df.drop_duplicates(subset='CustomerID')\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cdf69",
   "metadata": {},
   "source": [
    "In this section, we first loaded the data and then checked for missing values and filled them using K-Nearest Neighbors Imputation. We then checked for data integrity by looking at the data types of the columns, at the reasonable ranges of the data and the unique CustomerIDs.\n",
    "\n",
    "Now the data is ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d259363",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis **(EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c206e",
   "metadata": {},
   "source": [
    "## Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics for the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c4298",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for each column\n",
    "df.hist(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f08d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_copy = df.drop(columns='CustomerID')\n",
    "\n",
    "# Creating multiple boxplots, one for each column\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, col in enumerate(df_copy.columns):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.boxplot(x=df_copy[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a correlation matrix and visualize it with a heatmap\n",
    "corr = df_copy.corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbde401",
   "metadata": {},
   "source": [
    "We can see a quite high correlation between days_between_trans and repeat_share here (-0.67) and a higher correlation between days_between_trans and dog_share (0.41)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pairplots for all numerical data\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cff72",
   "metadata": {},
   "source": [
    "## Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a closer look at the most outstanding relationships\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='num_transactions', y='total_order_value', data=df)\n",
    "plt.title('num_transactions vs. total_order_value')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='days_between_trans', y='repeat_share', data=df)\n",
    "plt.title('days_between_trans vs. repeat_share')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='days_between_trans', y='dog_share', data=df)\n",
    "plt.title('dog_share vs. repeat_share')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='dog_share', y='repeat_share', data=df)\n",
    "plt.title('dog_share vs. repeat_share')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987825f",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling Features\n",
    "# Create a copy of the dataframe\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_scaled[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']] = scaler.fit_transform(df[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']])\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary statistics of the scaled data\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02b6d7",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (optional - check model results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1357a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation between days_between_trans and repeat_share is quite high, so it might be good to use PCA to reduce the dimensionality of the data\n",
    "# The problem is the interpretation of the data. Maybe simply take the two and make one out of them with PCA and keep the other 4\n",
    "\n",
    "# Use PCA to reduce the dimensionality of the data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize the PCA\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_pca = pca.fit_transform(df_scaled[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']])\n",
    "df_pca = pd.DataFrame(data=df_pca, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the explained variance ratio\n",
    "print('Explained Variance Ratio:', pca.explained_variance_ratio_)\n",
    "\n",
    "# Concatenate the PCA components with the original data\n",
    "df_final = pd.concat([df_scaled['CustomerID'], df_pca], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4b586",
   "metadata": {},
   "source": [
    "# Clustering and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Try out more Cluster algorithms and see which one fits the best\n",
    "#TODO: Try out more Dimensionality Reduction algorithms and see which one fits the best\n",
    "#TODO: Try to generate more features and see if the model improves\n",
    "#TODO: Try different number of clusters to find a better optimum (Elbow Method or Silhouette Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294d08c",
   "metadata": {},
   "source": [
    "We can use a 3 cluster segmentation in which we describe a high quality, medium quality and low quality customer.\n",
    "\n",
    "The high quality customer is a customer that has a high number of transactions, a high total order value, a low days between transactions, a high repeat share and a high dog share.\n",
    "\n",
    "The low and medium quality customer accordingly. We create a lead score for each customer based on the above features and then segment the customers into 3 clusters.\n",
    "\n",
    "We can then adjust our marketing strategy to target the high quality customers more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730df6ae",
   "metadata": {},
   "source": [
    "## Choosing the Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try the k-means clustering algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a copy of the final dataframe and drop the CustomerID column\n",
    "df_scaled_cluster = df_scaled.copy().drop('CustomerID', axis=1)\n",
    "\n",
    "# Initialize the KMeans algorithm\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fit the algorithm to the data\n",
    "df_scaled_cluster['Cluster'] = kmeans.fit_predict(df_scaled_cluster)\n",
    "df_scaled_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671ef33",
   "metadata": {},
   "source": [
    "# Evaluation of Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0c0da",
   "metadata": {},
   "source": [
    "## Analyzing Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the clusters\n",
    "cluster_stats = df_scaled_cluster.groupby('Cluster').mean()\n",
    "cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of customers in each cluster\n",
    "cluster_size = df_scaled_cluster['Cluster'].value_counts().reset_index()\n",
    "cluster_size.columns = ['Cluster', 'Count']\n",
    "\n",
    "# Calculate the distribution of each cluster\n",
    "cluster_dist = cluster_size['Count'] / cluster_size['Count'].sum()\n",
    "cluster_size['Distribution'] = cluster_dist\n",
    "cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3285893",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4dd7a",
   "metadata": {},
   "source": [
    "### Calculating Commonly Used Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(df_scaled_cluster.drop('Cluster', axis=1), df_scaled_cluster['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0af46",
   "metadata": {},
   "source": [
    "A score of more than 0.5 indicates a high-quality cluster. In our case it of course depends on the application of our clusters. If we are looking for a small number of high-quality customers, the results indicate that we could have already found them. Lets check the results further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7479924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Davies-Bouldin Index\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(df_scaled_cluster.drop('Cluster', axis=1), df_scaled_cluster['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20644529",
   "metadata": {},
   "source": [
    "### Visualizing the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14226cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='repeat_share', y='dog_share', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='num_transactions', y='total_order_value', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='total_order_value', y='days_between_trans', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355068a7",
   "metadata": {},
   "source": [
    "We can clearly see already that the clustering gives us a good segmentation of the customers. It is especially helpful to find the high quality customers that we want to explicitly target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Cluster column to the final dataframe\n",
    "df_final['Cluster'] = df_scaled_cluster['Cluster']\n",
    "\n",
    "# Visualize the cluster plots using PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_final, palette='viridis')\n",
    "plt.title('KMeans Clustering Results with PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1715421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the cluster plots using t-SNE\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Initialize the t-SNE algorithm\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# # Fit and transform the data\n",
    "# df_tsne = tsne.fit_transform(df_scaled_cluster.drop('Cluster', axis=1))\n",
    "\n",
    "# # Create a dataframe with the t-SNE components\n",
    "# df_tsne = pd.DataFrame(data=df_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "\n",
    "# # Concatenate the t-SNE components with the cluster column\n",
    "# df_tsne = pd.concat([df_tsne, df_scaled_cluster['Cluster']], axis=1)\n",
    "\n",
    "# # Visualize the cluster plots using t-SNE\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster', data=df_tsne, palette='viridis')\n",
    "# plt.title('KMeans Clustering Results with t-SNE')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='total_order_value', z='days_between_trans', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='repeat_share', z='dog_share', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d850d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='days_between_trans', z='repeat_share', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ed219",
   "metadata": {},
   "source": [
    "In this section we closely looked at the clusters and the characteristics of the customers in each cluster. We also validated the model by calculating commonly used scores and visualizing the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587ea1f",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_scaled['CustomerID'] with df_scaled_cluster['Cluster']\n",
    "df_clustered = pd.concat([df_scaled['CustomerID'], df_scaled_cluster['Cluster']], axis=1)\n",
    "df_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustered data to a CSV file\n",
    "df_clustered.to_csv('Clustered_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fedc5",
   "metadata": {},
   "source": [
    "# Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce315c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make create a Report that looks professional and is easy to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f5930",
   "metadata": {},
   "source": [
    "## Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15adef",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
