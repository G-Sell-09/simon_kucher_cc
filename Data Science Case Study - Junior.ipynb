{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f14d9a-8893-449a-930e-f5d946957a2c",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "***alles um tier GmBH*** is a pet supplies company. They are currently auditing their promotional activities and the CEO, one of the main stakeholders, feels that the promotions they offer is too generic and not targeted. They have requested us to devise a customer segmentation model that they can use to run targeted promotional activities.\n",
    "\n",
    "The client is interested in seeing what kind of customers are buying at ***alles um tier GmbH***. They assume that, in addition to private individuals, there are also smaller companies that purchase from ***alles um tier GmBH***. The project scope is to build a segmentation model and analyze the resulting customer segments.\n",
    "\n",
    "# Data\n",
    "\n",
    "You are given a dataset at customer level for the past year with the following data points. Number of transactions in the past year (*num_transactions*), order amount the past year (*total_order_value*), days between transactions the past year (*days_between_trans*), re-order rate the past year (*repeat_share*), and % of dog products bought (*dog_share*).\n",
    "\n",
    "### Data Set\n",
    "The dataset consists of 100k rows and has the following columns:\n",
    "\n",
    "* CustomerID (int): UUID for the customer\n",
    "* num_transactions (int): number of transactions in a given year\n",
    "* total_order_value (float): total order value in â‚¬ for the time period\n",
    "* days_between_trans (float): average days between transactions for a user\n",
    "* repeat_share (float): product share repeated every order\n",
    "* dog_share (float): percentage of products ordered that are dog food related\n",
    "    \n",
    "# Technical Environment\n",
    "* Python\n",
    "* numpy\n",
    "* pandas\n",
    "* scikit-learn\n",
    "* matplotlib / scipy / searborn / altair / plotly\n",
    "\n",
    "# Approach\n",
    "The solution is assessed on the following skills:\n",
    "* A thorough evaluation of the data set using statistical measures and visualization\n",
    "* Elegant Python coding skills\n",
    "* Machine learning modelling fundamentals\n",
    "* Model & result evaluation\n",
    "\n",
    "# Output\n",
    "Please provide your solution in a jupyter notebook with clear markdown comments.\n",
    "The final output should be in the form of a DataFrame with two columns, the CustomerId and the assigned cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acb833",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aec78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a docker container and send that one back so that there are no troubles for a customer to install the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22559e30",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import altair as alt\n",
    "import plotly.express as px\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f087c59",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example what the data to work with looks like. Delimiter is |\n",
    "# CustomerID|num_transactions|total_order_value|days_between_trans|repeat_share|dog_share\n",
    "# dwa726|12|329.88|33.77|0.459759531109544|0.255687053101122\n",
    "# asy963|1|11.28|234.14|0.0903120997560214|0.549722127878268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed64ab0-ad74-42b4-a516-857d513a1f2e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load the data and make sure to specify the correct delimiter\n",
    "df = pd.read_csv(\"DataSet_JuniorCodingChallenge.csv\", delimiter='|')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727eda2",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide in how to handle missing values \n",
    "# Due to the low number of missing values \n",
    "#TODO: Find a multivariate distribution and in cases where there is only one missing value of a Customer, compute the missing value with the distribution\n",
    "\n",
    "# Drop the missing values\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18558a30",
   "metadata": {},
   "source": [
    "## Data Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303900ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns are of the correct data type\n",
    "df.dtypes\n",
    "\n",
    "# Make sure to have the following data types:\n",
    "# CustomerID as a string\n",
    "# num_transactions as integer\n",
    "# total_order_value, days_between_trans, repeat_share, dog_share as floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58834216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73badef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the same function but also return the total number of values like this\n",
    "def find_floats(df, column):\n",
    "    count = 0\n",
    "    for i in df[column].unique():\n",
    "        if i % 1 != 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Find all float entries that are not .00 so typical integer values\n",
    "print(find_floats(df, 'total_order_value'))\n",
    "\n",
    "# There are 18249 entries that needs to be rounded at first before converting them to integers\n",
    "df['total_order_value'] = df['total_order_value'].apply(lambda x: round(x))\n",
    "\n",
    "# Double Check if all values are rounded now\n",
    "find_floats(df, 'total_order_value')\n",
    "\n",
    "# Convert all columns to integers\n",
    "df['num_transactions'] = df['num_transactions'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3adff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_order_value as a float\n",
    "df['total_order_value'] = df['total_order_value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ae8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all types again\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafd3d0",
   "metadata": {},
   "source": [
    "## Data Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69471e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of negative values for num_transactions, total_order_value and days_between_trans for negative values\n",
    "print(len(df[df['num_transactions'] < 0]))\n",
    "print(len(df[df['total_order_value'] < 0]))\n",
    "print(len(df[df['days_between_trans'] < 0]))\n",
    "\n",
    "# Check repeat_share and dog_share for values between 0 and 1. So count the number of values outside of this range\n",
    "print(len(df[(df['repeat_share'] < 0) | (df['repeat_share'] > 1)]))\n",
    "print(len(df[(df['dog_share'] < 0) | (df['dog_share'] > 1)]))\n",
    "\n",
    "# Drop the negative values in the 3 columns\n",
    "df = df[df['num_transactions'] >= 0]\n",
    "df = df[df['total_order_value'] >= 0]\n",
    "df = df[df['days_between_trans'] >= 0]\n",
    "\n",
    "# Drop the values outside of the range 0 and 1 for the last two columns\n",
    "df = df[(df['repeat_share'] >= 0) & (df['repeat_share'] <= 1)]\n",
    "df = df[(df['dog_share'] >= 0) & (df['dog_share'] <= 1)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0564692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated CustomerIDs\n",
    "df['CustomerID'].duplicated().sum()\n",
    "\n",
    "# Drop the duplicated CustomerIDs\n",
    "df = df.drop_duplicates(subset='CustomerID')\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cdf69",
   "metadata": {},
   "source": [
    "In this section, we first loaded the data and then checked for missing values. We then checked for data integrity by looking at the data types of the columns, at the reasonable ranges of the data and the unique CustomerIDs.\n",
    "Now the data is ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d259363",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis **(EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c206e",
   "metadata": {},
   "source": [
    "## Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics for the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c4298",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for each column\n",
    "df.hist(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f08d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a boxplot for each column in the dataframe\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Boxplot of Data Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix and visualize it with a heatmap\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create pairplots for the data\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cff72",
   "metadata": {},
   "source": [
    "## Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a closer look at the most outstanding relationships\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='num_transactions', y='total_order_value', data=df)\n",
    "plt.title('num_transactions vs. total_order_value')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='days_between_trans', y='repeat_share', data=df)\n",
    "plt.title('days_between_trans vs. repeat_share')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='dog_share', y='repeat_share', data=df)\n",
    "plt.title('dog_share vs. repeat_share')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scipy to calculate the p-value for the correlation between num_transactions and total_order_value\n",
    "p_value = stats.pearsonr(df['num_transactions'], df['total_order_value'])[1]\n",
    "print('P-value for num_transactions and total_order_value:', p_value)\n",
    "\n",
    "# Use scipy to calculate the p-value for the correlation between days_between_trans and repeat_share\n",
    "p_value = stats.pearsonr(df['days_between_trans'], df['repeat_share'])[1]\n",
    "print('P-value for days_between_trans and repeat_share:', p_value)\n",
    "\n",
    "# Use scipy to calculate the p-value for the correlation between dog_share and repeat_share\n",
    "p_value = stats.pearsonr(df['dog_share'], df['repeat_share'])[1]    \n",
    "print('P-value for dog_share and repeat_share:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987825f",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling Features\n",
    "# Create a copy of the dataframe\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_scaled[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']] = scaler.fit_transform(df[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']])\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary statistics of the scaled data\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02b6d7",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (optional - check model results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1357a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation between days_between_trans and repeat_share is quite high, so it might be good to use PCA to reduce the dimensionality of the data\n",
    "# The problem is the interpretation of the data. Maybe simply take the two and make one out of them with PCA and keep the other 4\n",
    "\n",
    "# Use PCA to reduce the dimensionality of the data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize the PCA\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_pca = pca.fit_transform(df_scaled[['num_transactions', 'total_order_value', 'days_between_trans', 'repeat_share', 'dog_share']])\n",
    "df_pca = pd.DataFrame(data=df_pca, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the explained variance ratio\n",
    "print('Explained Variance Ratio:', pca.explained_variance_ratio_)\n",
    "\n",
    "# Concatenate the PCA components with the original data\n",
    "df_final = pd.concat([df_scaled['CustomerID'], df_pca], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4b586",
   "metadata": {},
   "source": [
    "# Clustering and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Try out more Cluster algorithms and see which one fits the best\n",
    "#TODO: Try out more Dimensionality Reduction algorithms and see which one fits the best\n",
    "#TODO: Try to generate more features and see if the model improves\n",
    "#TODO: Try different number of clusters to find a better optimum (Elbow Method or Silhouette Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294d08c",
   "metadata": {},
   "source": [
    "We can use a 3 cluster segmentation in which we describe a high quality, medium quality and low quality customer.\n",
    "\n",
    "The high quality customer is a customer that has a high number of transactions, a high total order value, a low days between transactions, a high repeat share and a high dog share.\n",
    "\n",
    "The low and medium quality customer accordingly. We create a lead score for each customer based on the above features and then segment the customers into 3 clusters.\n",
    "\n",
    "We can then adjust our marketing strategy to target the high quality customers more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730df6ae",
   "metadata": {},
   "source": [
    "## Choosing the Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try the k-means clustering algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a copy of the final dataframe and drop the CustomerID column\n",
    "df_scaled_cluster = df_scaled.copy().drop('CustomerID', axis=1)\n",
    "\n",
    "# Initialize the KMeans algorithm\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fit the algorithm to the data\n",
    "df_scaled_cluster['Cluster'] = kmeans.fit_predict(df_scaled_cluster)\n",
    "df_scaled_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671ef33",
   "metadata": {},
   "source": [
    "# Evaluation of Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0c0da",
   "metadata": {},
   "source": [
    "## Analyzing Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the clusters\n",
    "cluster_stats = df_scaled_cluster.groupby('Cluster').mean()\n",
    "cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of customers in each cluster\n",
    "cluster_size = df_scaled_cluster['Cluster'].value_counts().reset_index()\n",
    "cluster_size.columns = ['Cluster', 'Count']\n",
    "\n",
    "# Calculate the distribution of each cluster\n",
    "cluster_dist = cluster_size['Count'] / cluster_size['Count'].sum()\n",
    "cluster_size['Distribution'] = cluster_dist\n",
    "cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3285893",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4dd7a",
   "metadata": {},
   "source": [
    "### Calculating Commonly Used Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(df_scaled_cluster.drop('Cluster', axis=1), df_scaled_cluster['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0af46",
   "metadata": {},
   "source": [
    "A score of more than 0.5 indicates a high-quality cluster. In our case it of course depends on the application of our clusters. If we are looking for a small number of high-quality customers, the results indicate that we could have already found them. Lets check the results further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7479924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Davies-Bouldin Index\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(df_scaled_cluster.drop('Cluster', axis=1), df_scaled_cluster['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20644529",
   "metadata": {},
   "source": [
    "### Visualizing the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14226cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='repeat_share', y='dog_share', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='num_transactions', y='total_order_value', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results of the clustering by using the Cluster column and the num_transactions and total_order_value columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='total_order_value', y='days_between_trans', hue='Cluster', data=df_scaled_cluster, palette='viridis')\n",
    "plt.title('KMeans Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355068a7",
   "metadata": {},
   "source": [
    "We can clearly see already that the clustering gives us a good segmentation of the customers. It is especially helpful to find the high quality customers that we want to explicitly target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cluster plots using PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_final, palette='viridis')\n",
    "plt.title('KMeans Clustering Results with PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1715421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cluster plots using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_tsne = tsne.fit_transform(df_scaled_cluster.drop('Cluster', axis=1))\n",
    "\n",
    "# Create a dataframe with the t-SNE components\n",
    "df_tsne = pd.DataFrame(data=df_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "\n",
    "# Concatenate the t-SNE components with the cluster column\n",
    "df_tsne = pd.concat([df_tsne, df_scaled_cluster['Cluster']], axis=1)\n",
    "\n",
    "# Visualize the cluster plots using t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster', data=df_tsne, palette='viridis')\n",
    "plt.title('KMeans Clustering Results with t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='total_order_value', z='days_between_trans', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='repeat_share', z='dog_share', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d850d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to create a 3D scatter plot of the clusters\n",
    "fig = px.scatter_3d(df_scaled_cluster, x='num_transactions', y='days_between_trans', z='repeat_share', color='Cluster', opacity=0.7)\n",
    "fig.update_layout(title='KMeans Clustering Results in 3D')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ed219",
   "metadata": {},
   "source": [
    "In this section we closely looked at the clusters and the characteristics of the customers in each cluster. We also validated the model by calculating commonly used scores and visualizing the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587ea1f",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_scaled['CustomerID'] with df_scaled_cluster['Cluster']\n",
    "df_clustered = pd.concat([df_scaled['CustomerID'], df_scaled_cluster['Cluster']], axis=1)\n",
    "df_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustered data to a CSV file\n",
    "df_clustered.to_csv('Clustered_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fedc5",
   "metadata": {},
   "source": [
    "# Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce315c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make create a Report that looks professional and is easy to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f5930",
   "metadata": {},
   "source": [
    "## Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15adef",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
